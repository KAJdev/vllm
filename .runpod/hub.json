{
  "title": "vLLM",
  "description": "A high-throughput and memory-efficient inference and serving engine for LLMs",
  "type": "serverless",
  "category": "language",
  "iconUrl": "https://registry.npmmirror.com/@lobehub/icons-static-png/latest/files/dark/vllm-color.png",
  "config": {
    "runsOn": "GPU",
    "gpuIds": "ADA_80_PRO",
    "containerDiskInGb": 100,
    "env": [
      {
        "key": "MODEL_NAME",
        "input": {
          "type": "huggingface",
          "name": "Model Name",
          "description": "The model you want to run"
        }
      }
    ]
  }
}
